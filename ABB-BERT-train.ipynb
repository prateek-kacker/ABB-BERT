{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dict_cont = pickle.load(open('dict/dict_cont', 'rb'))\n",
    "dict_abb = pickle.load(open('dict/dict_abb', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Dir runs/20210514-141227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "# \n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'runs/' + current_time \n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "print(\"Log Dir\",log_dir)\n",
    "load_model=True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, AdamW, BertConfig\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "\n",
    "\n",
    "from transformers.activations import gelu, gelu_new\n",
    "#from transformers.configuration_bert import BertConfig\n",
    "from transformers.file_utils import add_start_docstrings\n",
    "from transformers.modeling_utils import PreTrainedModel, prune_linear_layer\n",
    "\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def rank_fn(logs):\n",
    "    a,b=logs.sort(1,True)\n",
    "    c=np.where(b.cpu().numpy()==0)\n",
    "    return c[1]\n",
    "\n",
    "def tokenize_sent_map_to_ids(sentence, MAX_LEN = 128 ):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    \n",
    "    '''a function that takes a dataframe df containing columns CONTENT and class for sentences and labels respectively.\n",
    "    returns input id's and attention mask '''\n",
    "    \n",
    "    input_ids = []\n",
    "    changed_input_ids = []\n",
    "\n",
    "\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "def tokenize_sent1_sent2_map_to_ids(sentence1,sentence2=[], MAX_LEN = 512 ):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    \n",
    "    '''a function that takes a dataframe df containing columns CONTENT and class for sentences and labels respectively.\n",
    "    returns input id's and attention mask '''\n",
    "    \n",
    "\n",
    "    input_ids = []\n",
    "    if len(sentence2)!=0:\n",
    "        for sent1,sent2 in zip(sentence1,sentence2):\n",
    "            encoded_sent = tokenizer.encode(\n",
    "                                str(sent1),                      # Sentence to encode.\n",
    "                                str(sent2),\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                           )\n",
    "            input_ids.append(encoded_sent)\n",
    "    else:\n",
    "        for sent1 in sentence1:\n",
    "            encoded_sent = tokenizer.encode(\n",
    "                                str(sent1),                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                           )\n",
    "            input_ids.append(encoded_sent)\n",
    "    max_len=max([len(sen) for sen in input_ids])\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    if max_len < MAX_LEN:\n",
    "        MAX_LEN=max_len\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)\n",
    "\n",
    "#### LOAD BERT Tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('tokenizer.txt')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
    "    \"bert-base-uncased\": \"https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin\",\n",
    "    \"bert-large-uncased\": \"https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin\",\n",
    "    \"bert-base-cased\": \"https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin\",\n",
    "    \"bert-large-cased\": \"https://cdn.huggingface.co/bert-large-cased-pytorch_model.bin\",\n",
    "    \"bert-base-multilingual-uncased\": \"https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin\",\n",
    "    \"bert-base-multilingual-cased\": \"https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin\",\n",
    "    \"bert-base-chinese\": \"https://cdn.huggingface.co/bert-base-chinese-pytorch_model.bin\",\n",
    "    \"bert-base-german-cased\": \"https://cdn.huggingface.co/bert-base-german-cased-pytorch_model.bin\",\n",
    "    \"bert-large-uncased-whole-word-masking\": \"https://cdn.huggingface.co/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n",
    "    \"bert-large-cased-whole-word-masking\": \"https://cdn.huggingface.co/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\": \"https://cdn.huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\": \"https://cdn.huggingface.co/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
    "    \"bert-base-cased-finetuned-mrpc\": \"https://cdn.huggingface.co/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n",
    "    \"bert-base-german-dbmdz-cased\": \"https://cdn.huggingface.co/bert-base-german-dbmdz-cased-pytorch_model.bin\",\n",
    "    \"bert-base-german-dbmdz-uncased\": \"https://cdn.huggingface.co/bert-base-german-dbmdz-uncased-pytorch_model.bin\",\n",
    "    \"bert-base-japanese\": \"https://cdn.huggingface.co/cl-tohoku/bert-base-japanese/pytorch_model.bin\",\n",
    "    \"bert-base-japanese-whole-word-masking\": \"https://cdn.huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/pytorch_model.bin\",\n",
    "    \"bert-base-japanese-char\": \"https://cdn.huggingface.co/cl-tohoku/bert-base-japanese-char/pytorch_model.bin\",\n",
    "    \"bert-base-japanese-char-whole-word-masking\": \"https://cdn.huggingface.co/cl-tohoku/bert-base-japanese-char-whole-word-masking/pytorch_model.bin\",\n",
    "    \"bert-base-finnish-cased-v1\": \"https://cdn.huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/pytorch_model.bin\",\n",
    "    \"bert-base-finnish-uncased-v1\": \"https://cdn.huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1/pytorch_model.bin\",\n",
    "    \"bert-base-dutch-cased\": \"https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
    "    \"\"\" Load tf checkpoints in a pytorch model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import re\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(\n",
    "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
    "            for n in name\n",
    "        ):\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "                pointer = getattr(pointer, \"bias\")\n",
    "            elif scope_names[0] == \"output_weights\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"squad\":\n",
    "                pointer = getattr(pointer, \"classifier\")\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == \"_embeddings\":\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        elif m_name == \"kernel\":\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            assert pointer.shape == array.shape\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mish(x):\n",
    "    return x * torch.tanh(nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"gelu_new\": gelu_new, \"mish\": mish}\n",
    "\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification_notes: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification_notes from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification_notes from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification_notes were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.position_ids', 'classifier2.hidden.0.weight', 'classifier2.hidden.0.bias', 'classifier2.out.weight', 'classifier2.out.bias', 'personalization_layer.hidden.0.weight', 'personalization_layer.hidden.0.bias', 'personalization_layer.out.weight', 'personalization_layer.out.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model 570000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from os import path \n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for downloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, h_size, out_size, num_hid_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for k in range(num_hid_layers):\n",
    "            self.hidden.append(nn.Linear(h_size, h_size)) \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(h_size, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feedforward\n",
    "        for layer in self.hidden:\n",
    "            x = F.relu(layer(x))\n",
    "            \n",
    "        output= self.out(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MLP_1(nn.Module):\n",
    "    def __init__(self, h_size, out_size, num_hid_layers):\n",
    "        super(MLP_1, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for k in range(num_hid_layers):\n",
    "            self.hidden.append(nn.Linear(h_size, h_size)) \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(h_size, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         # Feedforward\n",
    "#         for layer in self.hidden:\n",
    "#             x = F.relu(layer(x))\n",
    "            \n",
    "        output= self.out(x)\n",
    "\n",
    "        return output\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdMSoftmaxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features,m=.8, s = 30):\n",
    "        '''\n",
    "        AM Softmax Loss\n",
    "        '''\n",
    "        super(AdMSoftmaxLoss, self).__init__()\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "\n",
    "    def forward(self, sentence, alternatives, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "\n",
    "\n",
    "        logits = torch.cosine_similarity(sentence,alternatives, dim = -1) \n",
    "        numerator = self.s *(torch.cosine_similarity(sentence,alternatives,dim=-1) - self.m * torch.tensor(labels).repeat(sentence.shape[0],1))\n",
    "        numerator=torch.exp(numerator)\n",
    "        denominator=torch.sum(numerator,1)\n",
    "        denominator=denominator.repeat((numerator.shape[1],1)).transpose(0,1)\n",
    "        loss=-torch.log(numerator/denominator)\n",
    "        return torch.mean(loss,1),numerator,logits\n",
    "    \n",
    "class BertForSequenceClassification_notes(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.classifier2 = MLP_1(config.hidden_size,config.hidden_size,1)\n",
    "        self.init_weights()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.personalization_layer = MLP_1(config.hidden_size,config.hidden_size,1)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        input_ids_alt = None,\n",
    "        attention_mask=None,\n",
    "        attention_mask_alt = None, \n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        train = True,\n",
    "        index_of_abb = None,\n",
    "        num_of_options=5,\n",
    "        alt_batch=10,\n",
    "        all_options=False,\n",
    "        all_alt_input_ids=False,\n",
    "        all_abb_bert=False,\n",
    "        just_cls=False,\n",
    "        all_alt_input_ids_vectors=[],\n",
    "        all_alt_abb_bert_vectors=[]\n",
    "    ):\n",
    " \n",
    "\n",
    "        outputs_1 = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "\n",
    "        if not just_cls:            \n",
    "            abb_indices = [l.item() for l in ((input_ids.flatten() == 1).nonzero()).flatten()]\n",
    "            abb_output = torch.stack([outputs_1[0][:,l,:] for l in abb_indices])\n",
    "            sent_int = abb_output\n",
    "            sent_int = self.dropout(sent_int)\n",
    "            sent_int2 = self.classifier2(sent_int)\n",
    "            sent_int2 = sent_int2.reshape((len(abb_indices),1,768))\n",
    "            sent=self.personalization_layer(sent_int2)\n",
    "        else:\n",
    "            sent_int =outputs_1[0][:,0,:]\n",
    "            sent_int = self.dropout(sent_int)\n",
    "            sent_int2 = self.classifier2(sent_int)\n",
    "            return sent_int2\n",
    "        \n",
    "        logits_list = []\n",
    "        if not all_options:\n",
    "            shapes_alts=input_ids_alt.shape\n",
    "\n",
    "            for alt_ids_index in range(0,shapes_alts[0],alt_batch):\n",
    "                outputs_2 = self.bert(\n",
    "                    input_ids = input_ids_alt[alt_ids_index:alt_ids_index+alt_batch,:],\n",
    "                    attention_mask = attention_mask_alt[alt_ids_index:alt_ids_index+alt_batch,:],\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    position_ids=position_ids,\n",
    "                    head_mask=head_mask,\n",
    "                    inputs_embeds=inputs_embeds)\n",
    "                if alt_ids_index==0:\n",
    "                    cls_output=outputs_2[0][:,0,:]\n",
    "                else:\n",
    "                    cls_output=torch.cat((cls_output,outputs_2[0][:,0,:]),dim=0)\n",
    "\n",
    "\n",
    "            criteria = AdMSoftmaxLoss(self.hidden_size,self.hidden_size)\n",
    "            altx = torch.stack([cls_output[i*num_of_options:(i+1)*num_of_options] for i in range(len(abb_indices))])\n",
    "\n",
    "            altx = self.classifier2(altx)\n",
    "            altx=self.personalization_layer(altx)\n",
    "            loss_1,logits,logits_list_test = criteria(sent, altx, labels )\n",
    "\n",
    "            logits_list.append(logits_list_test)\n",
    "            loss = loss_1\n",
    "\n",
    "\n",
    "        elif all_alt_input_ids: # When input ids are present\n",
    "            logits_list = []\n",
    "            for (all_alt_input_ids_vector,all_attention_mask_alt),label in all_alt_input_ids_vectors:\n",
    "                shapes_alts=all_alt_input_ids_vector.shape\n",
    "                for alt_ids_index in range(0,shapes_alts[0],alt_batch):\n",
    "                    outputs_2 = self.bert(\n",
    "                        input_ids = all_alt_input_ids_vector[alt_ids_index:alt_ids_index+alt_batch,:],\n",
    "                        attention_mask = all_attention_mask_alt[alt_ids_index:alt_ids_index+alt_batch,:],\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        position_ids=position_ids,\n",
    "                        head_mask=head_mask,\n",
    "                        inputs_embeds=inputs_embeds)\n",
    "                    if alt_ids_index==0:\n",
    "                        cls_output=outputs_2[0][:,0,:]\n",
    "                    else:\n",
    "                        cls_output=torch.cat((cls_output,outputs_2[0][:,0,:]),dim=0)\n",
    "\n",
    "                criteria = AdMSoftmaxLoss(self.hidden_size,self.hidden_size)\n",
    "                altx=cls_output\n",
    "                altx = self.classifier2(altx)\n",
    "                altx=self.personalization_layer(altx)\n",
    "                loss_1,logits,logits_list_test = criteria(sent, altx, label )\n",
    "                logits_list.append(logits_list_test)\n",
    "                loss = loss_1\n",
    "            \n",
    "        elif all_abb_bert: # when vectors from ABB_BERT is present\n",
    "            for num,(all_alt_abb_bert_vector,label) in enumerate(all_alt_abb_bert_vectors):\n",
    "                shapes_alts=all_alt_abb_bert_vector.shape\n",
    "                altx=all_alt_abb_bert_vector\n",
    "                criteria = AdMSoftmaxLoss(self.hidden_size,self.hidden_size)\n",
    "                altx=self.personalization_layer(altx)\n",
    "                print('altx shape',altx.shape)\n",
    "                print('sent shape',sent[num].shape)\n",
    "                loss,logits,logits_list_test = criteria(sent[num], altx, label)\n",
    "                print('numerator',logits_list_test.shape)\n",
    "                logits_list.append(logits_list_test)\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        return (loss), logits_list, logits_list_test,sent,sent_int2  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "model = BertForSequenceClassification_notes.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device(\"cuda:6\")\n",
    "number=0\n",
    "if load_model:\n",
    "    files_list=glob.glob(\"model/*\")\n",
    "    for file in files_list:\n",
    "        if number<int(file[6:]):\n",
    "            number=int(file[6:])\n",
    "    if path.exists(\"model/\"+str(number)):\n",
    "            model.load_state_dict(torch.load(\"model/\"+str(number)))\n",
    "            print(\"Loaded Model\",number)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "files_list = [l for l in os.listdir('dataset/') if 'train_' in l]\n",
    "random.shuffle(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 1000) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--/mnt/share/cds/AI_DEMO/envs/apex_env/lib/python3.7/site-packages/ipykernel_launcher.py:141: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train alt_size torch.Size([50, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/share/cds/AI_DEMO/envs/apex_env/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  0% (1 of 1000) |                       | Elapsed Time: 0:00:00 ETA:   0:16:06"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train alt_size torch.Size([100, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% (2 of 1000) |                       | Elapsed Time: 0:00:02 ETA:   0:18:17"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train alt_size torch.Size([100, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% (3 of 1000) |                       | Elapsed Time: 0:00:03 ETA:   0:18:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train alt_size torch.Size([200, 24])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-996093bbd1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;31m# Calculate elapsed time in minutes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mmse_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/share/cds/AI_DEMO/envs/apex_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/share/cds/AI_DEMO/envs/apex_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "optimizer2 =  AdamW(model.personalization_layer.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                   )\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "    \n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "loss_values_x_batch = []\n",
    "loss_values_x_batch_means = []\n",
    "logs_diff_values_x_batch = []\n",
    "logs_diff_values_x_batch_means = []\n",
    "ratio_less_10 = []\n",
    "ratio_less_50 = []\n",
    "ratio_less_100 =[]\n",
    "\n",
    "val_rank_list = []\n",
    "rank_list=[]\n",
    "\n",
    "logs_diff_values_validate = []\n",
    "correct_rank_validate = []\n",
    "num_of_options_val=50\n",
    "num_of_options_train=50\n",
    "mse_loss_list=[]\n",
    "# For each epoch...\n",
    "step=number\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    )\n",
    "for epoch_i in range(0, epochs):\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('epoch', epoch_i,step)\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "#     # For each batch of training data...\n",
    "    for file_index, file in enumerate(files_list):\n",
    "        with writer.as_default():\n",
    "            tf.summary.scalar('epoch*1000+file', epoch_i*1000+file_index,step)        \n",
    "        df = pickle.load(open('../tmp/'+file, 'rb'))\n",
    "        \n",
    "        # Total number of training steps is number of batches * number of epochs.\n",
    "        total_steps = len(df) * epochs\n",
    "        \n",
    "        # Create the learning rate scheduler.\n",
    "\n",
    "        for _, ind in enumerate(progressbar.progressbar(np.random.choice(len(df), len(df), replace=False))):\n",
    "            step+=1\n",
    "            batch = df[ind][0],df[ind][1]\n",
    "            batch_alt = df[ind][2],df[ind][3]\n",
    "            print(\"train alt_size\", batch_alt[0].shape)\n",
    "            if batch_alt[0].shape[0]>400 or batch_alt[0].shape[1]>40 or batch_alt[0].shape[0]*batch_alt[0].shape[1]> 6000 :\n",
    "                continue\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_input_ids_alt = batch_alt[0].to(device)\n",
    "            b_input_mask_alt = batch_alt[1].to(device)\n",
    "            b_labels = torch.zeros((num_of_options_train), dtype=torch.float) .to(device)\n",
    "            b_labels[0]=1.0\n",
    "\n",
    "            outputs = model( input_ids = b_input_ids,\n",
    "                            input_ids_alt = b_input_ids_alt,\n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            attention_mask_alt = b_input_mask_alt,\n",
    "                            labels=b_labels,\n",
    "                            num_of_options=num_of_options_train\n",
    "                           )\n",
    "\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logs = outputs[2]\n",
    "            sent_int2=outputs[4]\n",
    "            sent=outputs[3]\n",
    "            mse=torch.nn.MSELoss(size_average=None, reduce=None)\n",
    "            mse_loss=mse(sent,sent_int2)\n",
    "            rank_tensor=rank_fn(logs)\n",
    "            rank_list.append(np.mean(rank_tensor).item())\n",
    "            \n",
    "            total_loss += np.sum([l.item() for l in loss])\n",
    "            loss_values_x_batch.extend([l.item() for l in loss])\n",
    "\n",
    "            logs_diff_values_x_batch.extend([g.item() for l in logs for g in [l[0] - torch.mean(l[1:])] ])\n",
    "            loss = torch.mean(loss)\n",
    "            mse_loss_list.append(mse_loss.item())\n",
    "            if step % 1 == 0 :\n",
    "                loss.backward(retain_graph=True)\n",
    "                mse_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer2.step()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer2.zero_grad()\n",
    "                model.zero_grad()   \n",
    "\n",
    "                # Update the learning rate.\n",
    "                scheduler.step()\n",
    "\n",
    "            if step % 2 == 0 and not step == 0:\n",
    "\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('Mean of rank',np.mean(rank_list),step)\n",
    "                    tf.summary.scalar('log_diff',np.mean(logs_diff_values_x_batch),step)\n",
    "                    tf.summary.scalar('train_loss',np.mean(loss_values_x_batch),step)\n",
    "                    tf.summary.scalar('MSE Loss',np.mean(mse_loss_list),step)\n",
    "                loss_values_x_batch=[]\n",
    "                rank_list=[]\n",
    "                logs_diff_values_x_batch=[]\n",
    "            if step % 5000 == 0:\n",
    "                clear_output(wait=True)\n",
    "                torch.save(model.state_dict(), \"model/\"+str(step))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI_DEMO-apex_env] *",
   "language": "python",
   "name": "conda-env-AI_DEMO-apex_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
